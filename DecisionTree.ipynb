{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "* A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.\n",
        "* A decision tree starts with a root node, which does not have any incoming branches. The outgoing branches from the root node then feed into the internal nodes, also known as decision nodes. Based on the available features, both node types conduct evaluations to form homogenous subsets, which are denoted by leaf nodes, or terminal nodes. The leaf nodes represent all the possible outcomes within the dataset.\n",
        "\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "* entropy helps us to build an appropriate decision tree for selecting the best splitter. Entropy can be defined as a measure of the purity of the sub-split. Entropy always lies between 0 to 1. The entropy of any split can be calculated by this formula.\n",
        "H(s)=−P\n",
        "(+)\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " P\n",
        "(+)\n",
        "​\n",
        " −P\n",
        "(−)\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " P\n",
        "(−)\n",
        "​\n",
        " Here  \n",
        "P\n",
        "(−)\n",
        "​\n",
        "\n",
        "P\n",
        "(+)\n",
        "​\n",
        "\n",
        "​\n",
        " =% of positive class, and 1% of negative class.\n",
        " * The algorithm calculates the entropy of each feature after every split and as the splitting continues on, it selects the best feature and starts splitting according to it.\n",
        " * The graph of entropy increases up to 1 and then starts decreasing.\n",
        " * The range of entropy is from 0 to (log2C).\n",
        " * Gini Impurity of features after splitting can be calculated by using this formula. GI=1−∑\n",
        "i=1\n",
        "n\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "2\n",
        " GI=1−[(P\n",
        "(+)\n",
        "​\n",
        " )\n",
        "2\n",
        " +(P\n",
        "(−)\n",
        "​\n",
        " )\n",
        "2\n",
        " ]\n",
        " * Gini Impurity only goes up to 0.5 before decreasing, thus requiring less computational power.\n",
        " *the range of Gini Impurity is from 0 to 0.5 (for binary classification).\n",
        "\n",
        "\n",
        " Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "* Pre-pruning, also called early stopping, is a technique where you halt the construction of the decision tree before it fully grows. The idea is to prevent the tree from overfitting by limiting its size during the initial building phase.\n",
        "* Example: In areas like financial trading or online recommendation systems, where speed is critical, pre-pruning can be a game-changer. Let’s say you’re building a model to recommend products to users as they shop online. Here, milliseconds matter. You want a decision tree that can give quick predictions without being bogged down by too many complex branches. Pre-pruning helps you limit the depth of the tree so the model stays nimble, even if you lose a bit of accuracy.\n",
        "* Post-pruning, sometimes called cost-complexity pruning, works by allowing the decision tree to grow as much as it wants, capturing all the patterns (and possibly noise). After the tree is fully grown, we then come in and systematically remove the branches or splits that don’t contribute much to the model’s performance. Essentially, we’re simplifying the tree after the fact.\n",
        "* Example: In fields like medical diagnosis, the stakes are high. You want your model to be as accurate as possible, even if it means more computational work. Post-pruning allows you to first capture all potential interactions within the data, then simplify the tree to focus only on the branches that make meaningful contributions. For instance, in diagnosing diseases, the decision tree might initially grow very large, considering all sorts of patient symptoms and medical history, but post-pruning ensures that the tree only retains branches that genuinely impact the diagnosis.\n",
        "\n",
        "\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "* Information Gain (IG) is a measure used in decision trees to quantify the effectiveness of a feature in splitting the dataset into classes. It calculates the reduction in entropy (uncertainty) of the target variable (class labels) when a particular feature is known.\n",
        "* Decision trees aim to create subsets that are as pure as possible.\n",
        "* A split with higher IG reduces more uncertainty, meaning the feature provides more information about the class labels.\n",
        "* By selecting the feature with the highest IG at each node, the decision tree ensures efficient learning, smaller trees, and better classification performance.\n",
        "* Without IG, splits could be chosen arbitrarily and may not improve the predictive power of the model.\n",
        "\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "* Loan Approval in Banking: Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.\n",
        "* Medical Diagnosis: In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.\n",
        "* Fraud Detection: In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud. By analyzing past transaction data and patterns, Decision Trees can identify suspicious activities and flag them for further investigation.\n",
        "* Advantages:Easy to Understand,Versatility,No Need for Feature Scaling,Handles Non-linear Relationships,Handles Missing Data.\n",
        "* Limitations:Overfitting,Bias towards Features with Many Categories,Difficulty in Capturing Complex Interactions,Computationally Expensive for Large Datasets.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RkauzrY5jDiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances'''\n",
        "from sklearn.datasets import load_iris\n",
        "data=load_iris()\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(data.data,columns=data.feature_names)\n",
        "df['target']=data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x=df.iloc[:,:-1]\n",
        "y=df.iloc[:,-1]\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model=DecisionTreeClassifier(criterion='gini')\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U74V53-yKSZ",
        "outputId": "da59ab36-a488-4798-c22e-8ac5261ebdb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.'''\n",
        "from sklearn.datasets import load_iris\n",
        "data=load_iris()\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(data.data,columns=data.feature_names)\n",
        "df['target']=data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x=df.iloc[:,:-1]\n",
        "y=df.iloc[:,-1]\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "\n",
        "model=DecisionTreeClassifier()\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2y-cSs6z37J",
        "outputId": "e0708f6d-53e8-401f-aa62-7a3e9e30ba51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9666666666666667\n",
            "0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances'''\n",
        "import pandas as pd\n",
        "df=pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv')\n",
        "x=df.iloc[:,:-1]\n",
        "y=df.iloc[:,-1]\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "model=DecisionTreeRegressor()\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(mean_squared_error(y_test,y_pred))\n",
        "\n",
        "\n",
        "feature_importances = pd.DataFrame({ \"Feature\": x.columns, \"Importance\": model.feature_importances_ }).sort_values(by=\"Importance\", ascending=False)\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8Xgnz600LSG",
        "outputId": "17b2cbd0-7699-483d-be85-023046a211ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.756078431372547\n",
            "    Feature  Importance\n",
            "12    lstat    0.534187\n",
            "5        rm    0.255755\n",
            "7       dis    0.077248\n",
            "0      crim    0.040197\n",
            "4       nox    0.033416\n",
            "10  ptratio    0.020333\n",
            "6       age    0.011827\n",
            "2     indus    0.008660\n",
            "9       tax    0.007822\n",
            "11        b    0.005689\n",
            "3      chas    0.003115\n",
            "1        zn    0.001228\n",
            "8       rad    0.000523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy'''\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(dt, param_grid=params, cv=5, scoring='accuracy', verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "fD4_pphe1WPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4269707-dba2-4189-b09a-b56728c126aa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting\n",
        "* First i should understand the data.\n",
        "* Do EDA for the dataset.\n",
        "* Then apply df.isnull().sum() for checking whether any null value is present if present and replace it with mean,median,mode according the data.\n",
        "* Encode the data by using one hot encoding ,label encoding or target encoding etc. as per the categorical data type.\n",
        "* Then i will apply train test split.\n",
        "* Initialize Decision Tree classifier or regreesor as per the requirement ,then the Decision Tree algorithm will learn decision rules based on feature splits to predict the target variables.\n",
        "* Tune the models using techniques like Grid Search or Random Search with cross validation with the hyper parameters like max_depth,min_samples_split,criterion.\n",
        "* Find the best parameters and by applying that find the accuracy and evaluate its performance.\n",
        "\n",
        "\n",
        "This model will help in diagnosis and early detection,As per the disease the prevention and treatment etc."
      ],
      "metadata": {
        "id": "VMwvG__TLIhu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o67_LYmxN254"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}